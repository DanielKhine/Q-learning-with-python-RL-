{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Setting up Frozen Lake in code</h1>\n",
    "<h3><i>Libraries</i></h3>\n",
    "<p>First we’re importing all the libraries we’ll be using. Not many, really... Numpy, gym, random, time, and clear_output from Ipython’s display.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><i>Creating the environment</i></h3>\n",
    "<p>Next, to create our environment, we just call gym.make() and pass a string of the name of the environment we want to set up. We'll be using the environment FrozenLake-v0. All the environments with their corresponding names you can use here are available on Gym’s website. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With this <b><i>env</i></b> object, we’re able to query for information about the environment, sample states and actions, retrieve rewards, and have our agent navigate the frozen lake. That’s all made available to us conveniently with Gym.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sapce_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_sapce_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table.shape)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q-learning Algorithm</h3>\n",
    "<p>First, we create this list to hold all of the rewards we’ll get from each episode. This will be so we can see how our game score changes over time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><i>Update the Q-value</i></h3>\n",
    "\\begin{equation*} q^{new}\\left( s,a\\right) =\\left( 1-\\alpha \\right) ~\\underset{\\text{old value} }{\\underbrace{q\\left( s,a\\right) }\\rule[-0.05in]{0in}{0.2in} \\rule[-0.05in]{0in}{0.2in}\\rule[-0.1in]{0in}{0.3in}}+\\alpha \\overset{\\text{ learned value}}{\\overbrace{\\left(\n",
    "                                                    R_{t+1}+\\gamma \\max_{a^{^{\\prime }}}q\\left( s^{\\prime },a^{\\prime }\\right) \\right) }} \\end{equation*}\n",
    "<i>Eq 1.0</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Trainting Complete!****\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-table (eq 1.0)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "        learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        \n",
    "        # Add new reward\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "    # Exploration rate decay (using Exponential decay)\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "print(\"****Trainting Complete!****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.03200000000000002\n",
      "2000 :  0.19600000000000015\n",
      "3000 :  0.4030000000000003\n",
      "4000 :  0.5520000000000004\n",
      "5000 :  0.6500000000000005\n",
      "6000 :  0.6620000000000005\n",
      "7000 :  0.6710000000000005\n",
      "8000 :  0.6740000000000005\n",
      "9000 :  0.6830000000000005\n",
      "10000 :  0.6790000000000005\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.56193005 0.51904312 0.51133082 0.52203772]\n",
      " [0.27044836 0.26739031 0.31072775 0.50329229]\n",
      " [0.3879442  0.39202743 0.38405374 0.47641747]\n",
      " [0.3658019  0.38904336 0.37912939 0.46413361]\n",
      " [0.57738551 0.40079344 0.39364308 0.44860997]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.18362053 0.1287866  0.40630663 0.17050205]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.49886795 0.48726804 0.41940613 0.60020979]\n",
      " [0.44527007 0.65397234 0.47557004 0.23692219]\n",
      " [0.5568598  0.39871816 0.31294885 0.36951209]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.48380887 0.56428717 0.79754623 0.56847762]\n",
      " [0.74634644 0.9031563  0.71339461 0.71511925]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The code to watch the agent play the game</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action\n",
    "# from each state according to the Q-table\n",
    "\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode + 1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Show current state of environment on screen\n",
    "        clear_output(wait = True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # Choose action with highest Q-value for current state\n",
    "        action = np.argmax(q_table[state, :])\n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait = True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                # Agent recched the goal and won episode\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(2)\n",
    "                clear_output(wait = True)\n",
    "            else:\n",
    "                # Agent stepped in a hole and lost episode\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(2)\n",
    "                clear_output(wait = True)\n",
    "            break\n",
    "                \n",
    "        # Set new state\n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>References</b></p>\n",
    "<ul>\n",
    "<li><a href=\"http://deeplizard.com/learn/playlist/PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv\">DeepLizard</a>  [Reinforcement Learning - Introducing Goal Oriented Intelligence]</li>\n",
    "\n",
    "<li><a href=\"https://en.wikipedia.org/wiki/Q-learning\">Q-learning</a></li>\n",
    "    \n",
    "<li><a href=\"https://en.wikipedia.org/wiki/Exponential_Decay\">Exponential Decay</a></li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
